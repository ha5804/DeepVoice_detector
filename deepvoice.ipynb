{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 환경 설정 필요 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 위치 저장\n",
    "\n",
    "- 로컬 컴퓨터 맥북, mps 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Cofing\n",
    "\n",
    "### 실험에 필요한 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"data\": {\n",
    "        \"root\": \"/Users/taehayeong/Desktop/data/deepvoice_data/data\",\n",
    "        # 데이터 파일 경로\n",
    "        \"sample_rate\": 16000,\n",
    "        # 사람 음성 정보 가장 밀집된 대역, 일반적으로 16khz\n",
    "        \"n_mels\": 80,\n",
    "        \"segment_frames\": 300,\n",
    "        \"hop_length\": 256,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"hidden_dim\": 128,\n",
    "        \"latent_dim\": 64,\n",
    "        \"num_layers\": 2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 8,\n",
    "        \"lr\": 0.001,\n",
    "        \"epochs\": 20,\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Split\n",
    "\n",
    "### model에 따른 data 분할 방식 설정.\n",
    "\n",
    "- detection 모델 : trian = 오직 정상, test = 정상 + 딥보이스\n",
    "- classification 모델 : train , test = 정상 + 딥보이스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(model=\"anomaly\", root=None, seed=42):\n",
    "    random.seed(seed)\n",
    "    #실험 데이터 고정\n",
    "    speaker_dirs = [\n",
    "        os.path.join(root, d)\n",
    "        for d in os.listdir(root)\n",
    "        if d.isdigit() and os.path.isdir(os.path.join(root, d))\n",
    "    ]\n",
    "    #flac, 정상 파일의 폴더는 화자 단위의 번호로 되어있는 폴더. 화자 단위로 리스트 형성\n",
    "\n",
    "    random.shuffle(speaker_dirs)\n",
    "    train_speakers = speaker_dirs[:100]\n",
    "    test_speakers  = speaker_dirs[100:200]\n",
    "    # 화자 단위 list에서 다른 화자들로 구성.\n",
    "\n",
    "    def collect_files(speakers):\n",
    "        flacs, wavs = [], []\n",
    "        for spk in speakers:\n",
    "            for r, _, files in os.walk(spk):\n",
    "                for f in files:\n",
    "                    if f.endswith(\".flac\"):\n",
    "                        flacs.append(os.path.join(r, f))\n",
    "        wav_root = os.path.join(root, \"wavs\")\n",
    "        if os.path.isdir(wav_root):\n",
    "            for r, _, files in os.walk(wav_root):\n",
    "                for f in files:\n",
    "                    if f.endswith(\".wav\"):\n",
    "                        wavs.append(os.path.join(r, f))\n",
    "        return flacs, wavs\n",
    "    #flac은 정상, wavs는 비정상 파일로 구분\n",
    "\n",
    "    train_flac, train_wav = collect_files(train_speakers)\n",
    "    test_flac,  test_wav  = collect_files(test_speakers)\n",
    "\n",
    "    if model == \"anomaly\":\n",
    "        train = train_flac\n",
    "        test = [(p, 0) for p in test_flac] + [(p, 1) for p in test_wav]\n",
    "        random.shuffle(test)\n",
    "        return train, test\n",
    "    #detect 모델은 train에는 정상만, test는 비정상만 포함 & 라벨 부여.\n",
    "    elif model == \"classifier\":\n",
    "        train = [(p, 0) for p in train_flac] + [(p, 1) for p in train_wav]\n",
    "        test  = [(p, 0) for p in test_flac]  + [(p, 1) for p in test_wav]\n",
    "        random.shuffle(train)\n",
    "        random.shuffle(test)\n",
    "        n = len(train)\n",
    "        n_val = int(0.2 * n)\n",
    "        return train[:-n_val], train[-n_val:], test\n",
    "    #classifier 모델은 train test 둘다 정상 비정상 데이터 포함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessor & extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioPreprocessor:\n",
    "    def __init__(self, sr):\n",
    "        self.sr = sr\n",
    "    #config에서 저장한 samplate 받기\n",
    "    def preprocess(self, path):\n",
    "        wav, sr = librosa.load(path, sr=self.sr)\n",
    "        #wav는 1차원 numpy배열\n",
    "        return wav / (np.max(np.abs(wav)) + 1e-9)\n",
    "    #음성 파일 읽고, 파형으로 변환후 절대값을 통해 크기 확인,\n",
    "    #max를 통해 파형 전체를 가장 큰 값으로 나눔 -> [-1,1] 정규화\n",
    "    #소리 크기가 아닌 목소리 특징 학습을 위해 필요.\n",
    "    #1e+9로 0 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-mel spectro\n",
    "\n",
    "- sample rate(sr)는 초당 소리를 몇 칸으로 잘라서 저장하느냐이다. 즉, sr = 16000이라면, 1초 동안 소리를 16000번 찍는것이다.\n",
    "- wav : 파형은 소리가 흔들린 기록이다. 이 wav를 1초당 sr로 나눈다. \n",
    "- n_fft : 1초당 sr로 나눈것을 n_fft덩어리로 묶는다. 음성은 여러 시점에서의 소리를 합쳐 인식되기 때문에 필요.\n",
    "- hop_length : hop을 통해 하나의 음성에서 여러 분할로 소리가 시간에 따라 어떻게 변하는지 확인.\n",
    "- n_mels : 주파수 영역을 n_mels값으로 분할한것.\n",
    "- log : 소리 에너지를 log를 통해 작은 에너지도 확인 가능하게 한다.\n",
    "\n",
    "#### n_fft를 통해 주파수 분석의 해상도를, n_mels를 통해 사람이 듣는 방식으로 요약."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_logmel(wav, sr, n_mels):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=sr,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    return librosa.power_to_db(mel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dataset\n",
    "\n",
    "실제 모델 입력 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, file_list, preprocessor, feature_fn, segment_len, mode):\n",
    "        self.items = []\n",
    "        self.wav_cache = {}\n",
    "        #wav 한번만 읽기위해 사용\n",
    "        self.feature_fn = feature_fn\n",
    "        self.segment_len = segment_len\n",
    "        self.mode = mode\n",
    "\n",
    "        for wid, item in enumerate(file_list):\n",
    "            if mode == \"anomaly_train\":\n",
    "                path, label = item, 0\n",
    "            else:\n",
    "                path, label = item\n",
    "\n",
    "            wav = preprocessor.preprocess(path)\n",
    "            self.wav_cache[path] = wav\n",
    "\n",
    "            n_seg = len(wav) // segment_len\n",
    "            #통화 초반 몇 초를 여러 조각으로 나눔\n",
    "            for i in range(n_seg):\n",
    "                self.items.append((path, label, i, wid))\n",
    "            #segment 단위 item 생성\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, seg_idx, wid = self.items[idx]\n",
    "        wav = self.wav_cache[path]\n",
    "        seg = wav[seg_idx*self.segment_len:(seg_idx+1)*self.segment_len]\n",
    "        feat = self.feature_fn(seg)\n",
    "        #모델 입력 형태 (n_mes, T) T 는 시간 축.\n",
    "\n",
    "        if self.mode == \"anomaly_train\":\n",
    "            return feat\n",
    "        elif \"test\" in self.mode:\n",
    "            return feat, label, wid\n",
    "        else:\n",
    "            return feat, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model\n",
    "\n",
    "## 5.1 LSTM + AE\n",
    "\n",
    "- (1):init\n",
    "    - 모델 입력 변수 초기화\n",
    "- (2):encoder\n",
    "    - 음성 특징 학습\n",
    "- (3):latent\n",
    "    - encoder 결과 요약\n",
    "- (4):decoder\n",
    "    - 요약 결과 기존 형태 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels=80,\n",
    "        hidden_dim=128,\n",
    "        latent_dim=64,\n",
    "        num_layers=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- Encoder --------\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=n_mels,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # -------- Decoder --------\n",
    "        self.from_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=n_mels,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, n_mels, T)\n",
    "        return: reconstructed x (B, n_mels, T)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # -------- Encoder --------\n",
    "        enc_out, (h_n, _) = self.encoder(x)\n",
    "\n",
    "        h_last = h_n[-1]                 # (B, hidden_dim)\n",
    "        z = self.to_latent(h_last)       # (B, latent_dim)\n",
    "\n",
    "        # -------- Decoder --------\n",
    "        h_dec = self.from_latent(z)      # (B, hidden_dim)\n",
    "        T = x.size(1)\n",
    "        h_dec_seq = h_dec.unsqueeze(1).repeat(1, T, 1)\n",
    "\n",
    "        recon, _ = self.decoder(h_dec_seq)\n",
    "        recon = recon.permute(0, 2, 1)\n",
    "\n",
    "        return recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 CNN + BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) CNN : 주파수 - 시간 국소 패턴 추출\n",
    "- (2) BiLSTM : 발화 시간적 흐름 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepVoiceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels=80,\n",
    "        lstm_hidden=128,\n",
    "        num_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- CNN Encoder --------\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # -------- BiLSTM --------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(n_mels // 4) * 32,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # -------- Classifier --------\n",
    "        self.fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, n_mels, T)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.conv(x)\n",
    "\n",
    "        B, C, F, T = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, T, C, F)\n",
    "        x = x.reshape(B, T, C * F) # (B, T, feature)\n",
    "\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        logits = self.fc(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(scores, labels, title=\"ROC\"):\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Train LSTM + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, _ = get_data(\"anomaly\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "ae_dataset = VoiceDataset(\n",
    "    train_files, prep,\n",
    "    lambda x: torch.tensor(extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]), dtype=torch.float32),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"anomaly_train\"\n",
    ")\n",
    "\n",
    "ae_loader = DataLoader(ae_dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=True)\n",
    "#데이터 준비, tensor 변환\n",
    "\n",
    "ae = LSTMAutoEncoder(cfg[\"data\"][\"n_mels\"], cfg[\"model\"][\"hidden_dim\"],\n",
    "                     cfg[\"model\"][\"latent_dim\"], cfg[\"model\"][\"num_layers\"]).to(device)\n",
    "#model 생성\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=cfg[\"training\"][\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "#optimizer & Loss function\n",
    "ae_train_times = []\n",
    "#training 시간 저장\n",
    "for e in range(cfg[\"training\"][\"epochs\"]):\n",
    "    start = time.time()\n",
    "    for x in ae_loader:\n",
    "        x = x.to(device)\n",
    "        loss = loss_fn(ae(x), x)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    ae_train_times.append(time.time() - start)\n",
    "    print(f\"[AE Epoch {e+1}] loss={loss.item():.4f}\")\n",
    "#training- 복원 오차 계산, 더 잘 복원하도록 가중치 업데이트.\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "AE_CKPT_PATH = \"checkpoints/lstm_ae.pth\"\n",
    "torch.save(ae.state_dict(), AE_CKPT_PATH)\n",
    "#학습된 모델 저장.\n",
    "print(f\"AE model saved to {AE_CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Test LSTM + AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test = get_data(\"anomaly\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    test,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"anomaly_test\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "AE_CKPT_PATH = \"checkpoints/lstm_ae.pth\"\n",
    "\n",
    "model = LSTMAutoEncoder(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"],\n",
    "    cfg[\"model\"][\"latent_dim\"],\n",
    "    cfg[\"model\"][\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(AE_CKPT_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "wav_scores = defaultdict(list)\n",
    "wav_labels = {}\n",
    "\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, label, wid in loader:\n",
    "        x = x.to(device)\n",
    "        recon = model(x)\n",
    "        score = criterion(recon, x).mean().item()\n",
    "        wav_scores[wid.item()].append(score)\n",
    "        wav_labels[wid.item()] = label.item()\n",
    "\n",
    "total_test_time = time.time() - start_test\n",
    "print(\"Total AE test time:\", total_test_time)\n",
    "\n",
    "final_scores, final_labels = [], []\n",
    "for wid, scores in wav_scores.items():\n",
    "    final_scores.append(np.mean(sorted(scores, reverse=True)[:5]))\n",
    "    final_labels.append(wav_labels[wid])\n",
    "#wav안 특정 구간 가장 이상한 순간만 저장.\n",
    "\n",
    "auc = plot_roc(np.array(final_scores), np.array(final_labels))\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "np.save(\"ae_test_time.npy\", np.array([total_test_time]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train CNN + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, _ = get_data(\"classifier\", cfg[\"data\"][\"root\"])\n",
    "\n",
    "clf_dataset = VoiceDataset(\n",
    "    train, prep,\n",
    "    lambda x: torch.tensor(extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]), dtype=torch.float32).unsqueeze(0),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"classifier\"\n",
    ")\n",
    "\n",
    "clf_loader = DataLoader(clf_dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "clf = DeepVoiceClassifier(cfg[\"data\"][\"n_mels\"], cfg[\"model\"][\"hidden_dim\"]).to(device)\n",
    "opt = torch.optim.Adam(clf.parameters(), lr=cfg[\"training\"][\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "clf_train_times = []\n",
    "\n",
    "for e in range(cfg[\"training\"][\"epochs\"]):\n",
    "    start = time.time()\n",
    "    correct = total = 0\n",
    "    for x, y in clf_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = clf(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    clf_train_times.append(time.time() - start)\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    print(f\"[CLF Epoch {e+1}] acc={acc:.4f}\")\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "CLF_CKPT_PATH = \"checkpoints/classifier.pth\"\n",
    "torch.save(clf.state_dict(), CLF_CKPT_PATH)\n",
    "\n",
    "print(f\"Classifier model saved to {CLF_CKPT_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Test CNN + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, test = get_data(\"classifier\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    test,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(0),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"classifier_test\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "CLF_CKPT_PATH = \"checkpoints/classifier.pth\"\n",
    "\n",
    "model = DeepVoiceClassifier(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(CLF_CKPT_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wav_scores = defaultdict(list)\n",
    "wav_labels = {}\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, label, wid in loader:\n",
    "        x = x.to(device)\n",
    "        prob = torch.softmax(model(x), dim=1)[0, 1].item()\n",
    "        wav_scores[wid.item()].append(prob)\n",
    "        wav_labels[wid.item()] = label.item()\n",
    "\n",
    "total_test_time = time.time() - start_test\n",
    "print(\"Total classifier test time:\", total_test_time)\n",
    "\n",
    "np.save(\"clf_test_time.npy\", np.array([total_test_time]))\n",
    "\n",
    "\n",
    "final_scores, final_labels = [], []\n",
    "for wid, scores in wav_scores.items():\n",
    "    final_scores.append(np.mean(sorted(scores, reverse=True)[:5]))\n",
    "    final_labels.append(wav_labels[wid])\n",
    "\n",
    "auc = plot_roc(np.array(final_scores), np.array(final_labels))\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train & test time 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ae_train_times, label=\"AE Train\")\n",
    "plt.plot(clf_train_times, label=\"Classifier Train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit demo web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.models import LSTMAutoEncoder\n",
    "from src.utils import extract_logmel\n",
    "from src.preprocess import AudioPreprocessor  \n",
    "\n",
    "\n",
    "def split_audio(wav, segment_len):\n",
    "    n = len(wav) // segment_len\n",
    "    return [wav[i*segment_len:(i+1)*segment_len] for i in range(n)]\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model = LSTMAutoEncoder(\n",
    "        n_mels=80,\n",
    "        hidden_dim=128,\n",
    "        latent_dim=64,\n",
    "        num_layers=2\n",
    "    )\n",
    "    model.load_state_dict(\n",
    "        torch.load(\"checkpoints/lstm_ae.pth\", map_location=\"cpu\")\n",
    "    )\n",
    "    #저장된 학습 모델 로드\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "st.title(\"Deep Voice Protector\")\n",
    "\n",
    "uploaded = st.file_uploader(\"통화 음성 파일 업로드 (.wav/.flac)\", type=[\"wav\", \"flac\"])\n",
    "\n",
    "if uploaded and st.button(\"탐지 시작\"):\n",
    "    prep = AudioPreprocessor(sr=16000)\n",
    "    wav = prep.preprocess(uploaded)\n",
    "\n",
    "    segments = split_audio(\n",
    "        wav,\n",
    "        segment_len=300 * 256\n",
    "    )\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    with st.spinner(\"AI가 통화 초반 음성을 분석 중입니다...\"):\n",
    "        for seg in segments[:5]:\n",
    "            feat = torch.tensor(\n",
    "                extract_logmel(seg, sr=16000, n_mels = 80),\n",
    "                dtype=torch.float32\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                recon = model(feat)\n",
    "                err = torch.mean((recon - feat) ** 2).item()\n",
    "                errors.append(err)\n",
    "\n",
    "    score = float(np.mean(errors))\n",
    "    risk = min(score / 0.7, 1.0) * 100\n",
    "\n",
    "    st.subheader(\"탐지 결과\")\n",
    "    st.write(f\"딥보이스 의심 확률: **{risk:.1f}%**\")\n",
    "\n",
    "    if risk >= 70:\n",
    "        st.error(\"⚠ 딥보이스 위험 높음\\n\\n송금 보류 및 재확인 권장\")\n",
    "    else:\n",
    "        st.success(\"정상 음성으로 판단됨\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
