{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.config/config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data:\n",
    "  root: /Users/taehayeong/Desktop/data/deepvoice_data/data\n",
    "  sample_rate: 16000\n",
    "  n_mels: 80\n",
    "  segment_frames: 300   # frame 기준\n",
    "  hop_length: 256\n",
    "\n",
    "model:\n",
    "  hidden_dim: 128\n",
    "  latent_dim: 64\n",
    "  num_layers: 2\n",
    "\n",
    "training:\n",
    "  batch_size: 8\n",
    "  lr: 0.001\n",
    "  epochs: 20\n",
    "  save_path: checkpoints/lstm_ae.pth\n",
    "  classifier_save_path: checkpoints/classifier.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. src/data_split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def get_data(model=\"anomaly\", root=None, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    speaker_dirs = [\n",
    "        os.path.join(root, d)\n",
    "        for d in os.listdir(root)\n",
    "        if d.isdigit() and os.path.isdir(os.path.join(root, d))\n",
    "    ]\n",
    "\n",
    "    random.shuffle(speaker_dirs)\n",
    "\n",
    "    train_speakers = speaker_dirs[:100]\n",
    "    test_speakers  = speaker_dirs[100:200]\n",
    "\n",
    "    def collect_files(speakers):\n",
    "        flacs, wavs = [], []\n",
    "        for spk in speakers:\n",
    "            for r, _, files in os.walk(spk):\n",
    "                for f in files:\n",
    "                    if f.endswith(\".flac\"):\n",
    "                        flacs.append(os.path.join(r, f))\n",
    "        wav_root = os.path.join(root, \"wavs\")\n",
    "        if os.path.isdir(wav_root):\n",
    "            for r, _, files in os.walk(wav_root):\n",
    "                for f in files:\n",
    "                    if f.endswith(\".wav\"):\n",
    "                        wavs.append(os.path.join(r, f))\n",
    "        return flacs, wavs\n",
    "\n",
    "    train_flac, train_wav = collect_files(train_speakers)\n",
    "    test_flac,  test_wav  = collect_files(test_speakers)\n",
    "\n",
    "    # ---------------- ANOMALY ----------------\n",
    "    if model == \"anomaly\":\n",
    "        train = train_flac\n",
    "\n",
    "        test = (\n",
    "            [(p, 0) for p in test_flac] +\n",
    "            [(p, 1) for p in test_wav]\n",
    "        )\n",
    "        random.shuffle(test)\n",
    "\n",
    "        return train, test\n",
    "\n",
    "    # ---------------- CLASSIFIER ----------------\n",
    "    elif model == \"classifier\":\n",
    "        train = (\n",
    "            [(p, 0) for p in train_flac] +\n",
    "            [(p, 1) for p in train_wav]\n",
    "        )\n",
    "        test = (\n",
    "            [(p, 0) for p in test_flac] +\n",
    "            [(p, 1) for p in test_wav]\n",
    "        )\n",
    "\n",
    "        random.shuffle(train)\n",
    "        random.shuffle(test)\n",
    "\n",
    "        n = len(train)\n",
    "        n_val = int(0.2 * n)\n",
    "\n",
    "        return train[:-n_val], train[-n_val:], test\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"model must be 'anomaly' or 'classifier'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.src/dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, file_list, preprocessor, feature_fn, segment_len, mode):\n",
    "        self.items = []\n",
    "        self.wav_cache = {}\n",
    "        self.feature_fn = feature_fn\n",
    "        self.segment_len = segment_len\n",
    "        self.mode = mode\n",
    "\n",
    "        for wid, item in enumerate(file_list):\n",
    "            if mode == \"anomaly_train\":\n",
    "                path, label = item, 0\n",
    "            else:\n",
    "                path, label = item\n",
    "\n",
    "            wav = preprocessor.preprocess(path)\n",
    "            self.wav_cache[path] = wav\n",
    "\n",
    "            n_seg = len(wav) // segment_len\n",
    "            for i in range(n_seg):\n",
    "                self.items.append((path, label, i, wid))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label, seg_idx, wid = self.items[idx]\n",
    "        wav = self.wav_cache[path]\n",
    "\n",
    "        seg = wav[\n",
    "            seg_idx*self.segment_len:(seg_idx+1)*self.segment_len\n",
    "        ]\n",
    "\n",
    "        feat = self.feature_fn(seg)\n",
    "\n",
    "        if self.mode == \"anomaly_train\":\n",
    "            return feat\n",
    "        elif \"test\" in self.mode:\n",
    "            return feat, label, wid\n",
    "        else:\n",
    "            return feat, label\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.src/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_config(path):\n",
    "    with open(path) as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def extract_logmel(wav, sr, n_mels):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=sr,\n",
    "        n_fft=1024,\n",
    "        hop_length=256,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    return librosa.power_to_db(mel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.src/preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sr):\n",
    "        self.sr = sr\n",
    "\n",
    "    def preprocess(self, path):\n",
    "        wav, _ = librosa.load(path, sr=self.sr)\n",
    "        return wav / (np.max(np.abs(wav)) + 1e-9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels=80,\n",
    "        hidden_dim=128,\n",
    "        latent_dim=64,\n",
    "        num_layers=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- Encoder --------\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=n_mels,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # -------- Decoder --------\n",
    "        self.from_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=n_mels,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, n_mels, T)\n",
    "        return: reconstructed x (B, n_mels, T)\n",
    "        \"\"\"\n",
    "\n",
    "        # (B, n_mels, T) → (B, T, n_mels)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # -------- Encoder --------\n",
    "        enc_out, (h_n, _) = self.encoder(x)\n",
    "        # h_n: (num_layers, B, hidden_dim)\n",
    "\n",
    "        h_last = h_n[-1]                 # (B, hidden_dim)\n",
    "        z = self.to_latent(h_last)       # (B, latent_dim)\n",
    "\n",
    "        # -------- Decoder --------\n",
    "        h_dec = self.from_latent(z)      # (B, hidden_dim)\n",
    "\n",
    "        # repeat for each timestep\n",
    "        T = x.size(1)\n",
    "        h_dec_seq = h_dec.unsqueeze(1).repeat(1, T, 1)\n",
    "\n",
    "        recon, _ = self.decoder(h_dec_seq)\n",
    "        # recon: (B, T, n_mels)\n",
    "\n",
    "        # (B, T, n_mels) → (B, n_mels, T)\n",
    "        recon = recon.permute(0, 2, 1)\n",
    "\n",
    "        return recon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.model2.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/model2.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DeepVoiceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels=80,\n",
    "        lstm_hidden=128,\n",
    "        num_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- CNN Encoder --------\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        # -------- BiLSTM --------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=(n_mels // 4) * 32,\n",
    "            hidden_size=lstm_hidden,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # -------- Classifier --------\n",
    "        self.fc = nn.Linear(lstm_hidden * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, n_mels, T)\n",
    "        \"\"\"\n",
    "\n",
    "        # CNN\n",
    "        x = self.conv(x)\n",
    "        # (B, C, n_mels//4, T//4)\n",
    "\n",
    "        B, C, F, T = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, T, C, F)\n",
    "        x = x.reshape(B, T, C * F) # (B, T, feature)\n",
    "\n",
    "        # LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # 마지막 timestep\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.fc(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "from src.data_split import get_data\n",
    "from src.dataset import VoiceDataset\n",
    "from src.preprocess import AudioPreprocessor\n",
    "from src.utils import load_config, extract_logmel\n",
    "from src.models import LSTMAutoEncoder\n",
    "\n",
    "cfg = load_config(\"config/config.yaml\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "train_files, _ = get_data(\"anomaly\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    train_files,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"anomaly_train\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "model = LSTMAutoEncoder(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"],\n",
    "    cfg[\"model\"][\"latent_dim\"],\n",
    "    cfg[\"model\"][\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=cfg[\"training\"][\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n",
    "epoch_times = []\n",
    "\n",
    "for e in range(cfg[\"training\"][\"epochs\"]):\n",
    "    start = time.time()\n",
    "\n",
    "    for x in loader:\n",
    "        x = x.to(device)\n",
    "        loss = loss_fn(model(x), x)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    epoch_times.append(elapsed)\n",
    "\n",
    "    print(f\"[Epoch {e+1}] loss={loss.item():.4f}, time={elapsed:.2f}s\")\n",
    "\n",
    "total_train_time = sum(epoch_times)\n",
    "print(\"Total AE train time:\", total_train_time)\n",
    "\n",
    "np.save(\"ae_train_times.npy\", np.array(epoch_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# src/train2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "from src.data_split import get_data\n",
    "from src.dataset import VoiceDataset\n",
    "from src.preprocess import AudioPreprocessor\n",
    "from src.utils import load_config, extract_logmel\n",
    "from src.models2 import DeepVoiceClassifier\n",
    "\n",
    "cfg = load_config(\"config/config.yaml\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "train, _, _ = get_data(\"classifier\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    train,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(0),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"classifier\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=cfg[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "model = DeepVoiceClassifier(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=cfg[\"training\"][\"lr\"])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epoch_times = []\n",
    "\n",
    "for e in range(cfg[\"training\"][\"epochs\"]):\n",
    "    start = time.time()\n",
    "    correct = total = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss = loss_fn(model(x), y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    epoch_times.append(elapsed)\n",
    "\n",
    "    print(f\"[Epoch {e+1}] acc={correct/total:.4f}, time={elapsed:.2f}s\")\n",
    "\n",
    "np.save(\"clf_train_times.npy\", np.array(epoch_times))\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), cfg[\"training\"][\"classifier_save_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from src.data_split import get_data\n",
    "from src.dataset import VoiceDataset\n",
    "from src.preprocess import AudioPreprocessor\n",
    "from src.utils import load_config, extract_logmel\n",
    "from src.models import LSTMAutoEncoder\n",
    "from eval import plot_roc\n",
    "\n",
    "cfg = load_config(\"config/config.yaml\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "_, test = get_data(\"anomaly\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    test,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"anomaly_test\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = LSTMAutoEncoder(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"],\n",
    "    cfg[\"model\"][\"latent_dim\"],\n",
    "    cfg[\"model\"][\"num_layers\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(cfg[\"training\"][\"save_path\"], map_location=device))\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "wav_scores = defaultdict(list)\n",
    "wav_labels = {}\n",
    "\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, label, wid in loader:\n",
    "        x = x.to(device)\n",
    "        recon = model(x)\n",
    "        score = criterion(recon, x).mean().item()\n",
    "        wav_scores[wid.item()].append(score)\n",
    "        wav_labels[wid.item()] = label.item()\n",
    "\n",
    "total_test_time = time.time() - start_test\n",
    "print(\"Total AE test time:\", total_test_time)\n",
    "\n",
    "final_scores, final_labels = [], []\n",
    "for wid, scores in wav_scores.items():\n",
    "    final_scores.append(np.mean(sorted(scores, reverse=True)[:5]))\n",
    "    final_labels.append(wav_labels[wid])\n",
    "\n",
    "auc = plot_roc(np.array(final_scores), np.array(final_labels))\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "np.save(\"ae_test_time.npy\", np.array([total_test_time]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import numpy as np\n",
    "from src.data_split import get_data\n",
    "from src.dataset import VoiceDataset\n",
    "from src.preprocess import AudioPreprocessor\n",
    "from src.utils import load_config, extract_logmel\n",
    "from src.models2 import DeepVoiceClassifier\n",
    "from eval import plot_roc\n",
    "\n",
    "cfg = load_config(\"config/config.yaml\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "_, _, test = get_data(\"classifier\", cfg[\"data\"][\"root\"])\n",
    "prep = AudioPreprocessor(cfg[\"data\"][\"sample_rate\"])\n",
    "\n",
    "dataset = VoiceDataset(\n",
    "    test,\n",
    "    prep,\n",
    "    lambda x: torch.tensor(\n",
    "        extract_logmel(x, cfg[\"data\"][\"sample_rate\"], cfg[\"data\"][\"n_mels\"]),\n",
    "        dtype=torch.float32\n",
    "    ).unsqueeze(0),\n",
    "    cfg[\"data\"][\"segment_frames\"] * cfg[\"data\"][\"hop_length\"],\n",
    "    \"classifier_test\"\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = DeepVoiceClassifier(\n",
    "    cfg[\"data\"][\"n_mels\"],\n",
    "    cfg[\"model\"][\"hidden_dim\"]\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(cfg[\"training\"][\"classifier_save_path\"], map_location=device))\n",
    "model.eval()\n",
    "\n",
    "wav_scores = defaultdict(list)\n",
    "wav_labels = {}\n",
    "\n",
    "start_test = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, label, wid in loader:\n",
    "        x = x.to(device)\n",
    "        prob = torch.softmax(model(x), dim=1)[0, 1].item()\n",
    "        wav_scores[wid.item()].append(prob)\n",
    "        wav_labels[wid.item()] = label.item()\n",
    "\n",
    "total_test_time = time.time() - start_test\n",
    "print(\"Total classifier test time:\", total_test_time)\n",
    "\n",
    "np.save(\"clf_test_time.npy\", np.array([total_test_time]))\n",
    "\n",
    "\n",
    "final_scores, final_labels = [], []\n",
    "for wid, scores in wav_scores.items():\n",
    "    final_scores.append(np.mean(sorted(scores, reverse=True)[:5]))\n",
    "    final_labels.append(wav_labels[wid])\n",
    "\n",
    "auc = plot_roc(np.array(final_scores), np.array(final_labels))\n",
    "print(\"AUC:\", auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.eval.py / plot_distributiin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(scores, labels):\n",
    "    \"\"\"\n",
    "    scores: anomaly scores (higher = more anomalous)\n",
    "    labels: 0 (normal), 1 (fake)\n",
    "    \"\"\"\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"darkorange\",\n",
    "        lw=2,\n",
    "        label=f\"ROC curve (AUC = {roc_auc:.3f})\"\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (DeepVoice Anomaly Detection)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_score_distribution(scores, labels):\n",
    "    normal_scores = scores[labels == 0]\n",
    "    fake_scores = scores[labels == 1]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.hist(\n",
    "        normal_scores,\n",
    "        bins=50,\n",
    "        alpha=0.6,\n",
    "        label=\"Normal (Human)\",\n",
    "        density=True\n",
    "    )\n",
    "    plt.hist(\n",
    "        fake_scores,\n",
    "        bins=50,\n",
    "        alpha=0.6,\n",
    "        label=\"DeepVoice (Fake)\",\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Anomaly Score (Reconstruction Error)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Anomaly Score Distribution\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
